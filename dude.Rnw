\documentclass[mathserif]{beamer}
%\documentclass[serif,mathserif]{beamer}
%\mode<presentation>{ \usetheme{Madrid}\usecolortheme{seahorse}\setbeamertemplate{blocks}[rounded][shadow=TRUE] }
\mode<presentation>{ \usetheme{Pittsburgh}\usecolortheme{seahorse}\setbeamertemplate{blocks}[rounded][shadow=TRUE] }
%\mode<presentation>{ \usetheme{boxes} }

% for help on beamer, see e.g.
% http://heather.cs.ucdavis.edu/~matloff/beamer.html
% wget http://heather.cs.ucdavis.edu/~matloff/BeamerTour.tex
%
% beamer with 2-page sweave & code:
% http://stackoverflow.com/questions/6964750/two-column-beamer-sweave-slide-with-grid-graphic


<<'preamble',echo=FALSE,print=FALSE,warning=FALSE,message=FALSE>>=
source("knitr_opts.R")
source("defs.R")

compile.time <- Sys.time()

# compiler flags!

#FINAL.VERSION <- TRUE
FINAL.VERSION <- FALSE

# not used yet
LONG.FORM <- FALSE
@

<<'fama_french_sr'>>=
source("get_ff_data.R")
frow <- 1 + max(which(apply(is.na(ff_daily_factors),1,any)))
subrr <- ff_daily_factors[frow:dim(ff_daily_factors)[1],]
rr2lr <- function(x) {
	retval <- log(1+x)
	return(retval)
}
lr2rr <- function(x) { 
	retval <- exp(x) - 1
	return(retval)
}
subsrs <- apply(rr2lr(subrr[,c("smb","hml","mktrf","umd")]),2,f_asharpe)
@

%2FIX: put a header here.

\typeout{-- dude.tex}
\typeout{-- NC 2011-2013 s.e.p.}
\typeout{-- SVNId: $Id: $}

%TODO

%preamble%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%FOLDUP

%to make wikipedia bibtex entries happy
\usepackage{url}

%\usepackage{psfig}\psfull
%\usepackage{graphicx,epsfig,psfrag}
\usepackage{graphicx}
\usepackage{subfigure}
%\graphicspath{{figs/}}
\newcommand{\useBW}{.bw}
\newcommand{\figDIR}{figs}

%for pretty printing code;
\usepackage{listings}

\lstset{frame=topline,float=tbph,tabsize=2,numbers=left}

% here I go:
% http://www.ctan.org/tex-archive/graphics/pgf/base/doc/generic/pgf/pgfmanual.pdf
\usepackage{tikz}
\usetikzlibrary{shapes,arrows}


\usepackage[today,nofancy]{svninfo}
\svnInfo $Id: dude.Rnw 01 2013-02-08 20:59:08Z astrawman $

%\usepackage{alg}
%\floatstyle{boxed}
%\floatstyle{ruled}
%\restylefloat{algorithmfloat}
%\usepackage{empheq}
%\providecommand{\widefbox}[1]{\fbox{\hspace{1em}{#1}\hspace{1em}}}

%compactitem and such:
\usepackage[newitem,newenum,increaseonly]{paralist}
%importing:
%\usepackage{import}
%\import*{dirname/}{filenamenodottex}

%UNFOLD

% latex shortcuts%FOLDUP
%\usepackage[commands,shortcuts]{sepmath}
\input{sharpe_shortcuts.tex}

%for pretty printing code;
\usepackage{listings}

% do some overloading for the slide show such that
% sample statistics are in roman and parameters are 
% in Greek.  mostly.
\renewcommand{\psrUL}[2]{\mathUL{\zeta}{#1}{#2}}
\renewcommand{\psr}[1][]{\psrUL{}{#1}}
\renewcommand{\psnr}[1][]{\psrUL{}{#1}}
\renewcommand{\psrsq}[1][]{\psrUL{2}{#1}}
\renewcommand{\psnrsq}[1][]{\psrUL{2}{#1}}

% or \hat{\zeta}?
\renewcommand{\ssrUL}[2]{\mathUL{\hat{\zeta}}{#1}{#2}}
\renewcommand{\ssr}[1][]{\ssrUL{}{#1}}
\renewcommand{\ssrsq}[1][]{\ssrUL{2}{#1}}

% redefine sample stats to have all hats.
\renewcommand{\smu}[1][]{\mathSUB{\hat{\mu}}{#1}}
\renewcommand{\ssig}[1][]{\mathSUB{\hat{\sigma}}{#1}}

\renewcommand{\svmu}[1][]{\mathSUB{\hat{\vect{\mu}}}{#1}}
\renewcommand{\svsig}[1][]{\mathSUB{\hat{\Sigma}}{#1}}

% want to make this seem more like a constant
\renewcommand{\tpowc}[1][]{\mathSUB{\kappa}{#1}}

\providecommand{\stratrc}[1][]{\mathSUB{\theta}{#1}}
% see superuser.com/questions/69856
%\providecommand{\coindie}{\MATHIT{\coprod}}
\providecommand{\coindie}{\MATHIT{\perp\!\!\!\perp}}

% this is broken in beamer for some reason:
\renewcommand{\pvsig}[1][]{\mathSUB{\Sigma}{#1}}
%UNFOLD

\title[Dude...]{Dude, Where's my Alpha?}\subtitle{dispatches from a `quant'}%FOLDUP
\author[Steven E. Pav]{Steven E. Pav \\ \texttt{steven@cerebellumcapital.com}}
\institute{Cerebellum Capital}
%\date{\today}
%\date[UCSF 2013]{\Sexpr{format(compile.time,'%Y-%m-%d')}}
\date[\Sexpr{format(compile.time,'%Y-%m-%d')}]{UCSF \Sexpr{format(compile.time,'%Y-%m-%d')}}
\subject{Quantitative Finance}
%UNFOLD

% have this if you'd like a recurring outline
%\AtBeginSection[]  % "Beamer, do the following at the start of every section"
%{
%\begin{frame}<beamer> 
%\frametitle{Outline} % make a frame titled "Outline"
%\tableofcontents[currentsection]  % show TOC and highlight current section
%\end{frame}
%}

\begin{document}


\begin{frame}
\titlepage
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Intro}%FOLDUP

\begin{frame}{The Billion Dollar Quant Problem}%FOLDUP
Stipulate the following:
\begin{itemize}
\item Generating ideas for trading strategies is easy.\\
Generating \emph{good} ideas is harder.
\item Backtesting strategies is an engineering problem.\\
Not easy, not insurmountable: lots of possible biases, missing
data, some domain knowledge needed.
\item If evaluating strategies were easy, a lot of smart people would be rich.
\end{itemize}
Evaluating strategies must be hard. 
\end{frame}%UNFOLD

\begin{frame}{A Simple Example: Choose One}%FOLDUP

<<'wyrather',include=FALSE,fig=TRUE>>=
nyr <- 5
epy <- 253
nobs <- ceiling(nyr * epy)
tdf <- 5

#set.seed(92843)
#set.seed(12843)  # in this one, the gren line dominates at 10 years!
set.seed(72843)  
# sr and volatility, annualized.
downit <- 1.0
sr1 <- downit * 0.45
sr2 <- downit * 1.6

# force them to have (nearly) the same long term mean:
sg1 <- 0.19
mu1 <- sr1 * sg1
mu2 <- 0.60 * mu1
sg2 <- mu2 / sr2

m1 <- exp(cumsum((sg1 / sqrt(epy)) * ((sr1/sqrt(epy)) + sqrt((tdf - 2)/tdf) * rt(nobs,df=tdf))))
m2 <- exp(cumsum((sg2 / sqrt(epy)) * ((sr2/sqrt(epy)) + sqrt((tdf - 2)/tdf) * rt(nobs,df=tdf))))
yrs <- seq(from=1,to=1+nyr,length.out=nobs)

plot(yrs,m1,xlab="year",ylab="AUM",type='l',col='green',log='y')
lines(yrs,m2,col='blue')
legend(1 + 0.1*nyr,0.95 * max(max(m2),max(m1)), # the location
			 c("strategy 1","strategy 2"),  # what?
			 lty=c(1,1), # gives the legend appropriate symbols (lines)
			 col=c("green","blue"))

@
\vspace{-0.35in}
\begin{figure}[htbp]
  \begin{center}
		\includegraphics[width=2.5in]{figure/wyrather}
		%\caption{Which strategy would you rather invest in? ``Slow and Steady'' or ``Loose Cannon''?}
		\label{fig:wyrather}
	\end{center}
\end{figure}
\vspace{-0.20in}
Which strategy would you rather invest in? ``Slow and Steady'' or ``Loose Cannon''?
%\emph{n.b.} You don't get to replay history.
\end{frame}
%UNFOLD

%\begin{frame}[allowframebreaks]{the Sharpe Ratio}
\begin{frame}[allowframebreaks]{Basic Performance Analysis}%FOLDUP
\begin{itemize}
\item Let \pryt[t] be the `mark-to-market' (MtM) at time $t$.
\item Compute the \emph{returns}:
\begin{align*}
\mbox{Geometric:}\,\,\grett[t] &\defeq \log\frac{\pryt}{\pryt[t-1]},\\
\mbox{Arithmetic:}\,\,\arett[t] &\defeq \frac{\pryt}{\pryt[t-1]} - 1 = \exp{\grett[t]} - 1.
\end{align*}
By simple calculus: $\grett[t] \le \arett[t]$ with equality at zero.
\item Sequential Geometric returns are additive by telescoping.
\item Contemporaneous Arithmetic returns are additive:\\
Arithmetic return of a portfolio is the dollar-weighted
average of the components' arithmetic returns. (This includes `shorting'.)

\break

\item Let $\pmu = \E{\grett[]}, \psigsq = \VAR{\grett[]}$.\\
Throughout, mostly assume \grett[t] are \iid (unrealistic).
\item For fixed \pmu, prefer a smaller \psig; for fixed
\psig prefer larger \pmu.\\
The \emph{efficient frontier} is the set of optimal strategies
under this preference:
\end{itemize}
%\begin{center}%FOLDUP
	%\vspace{-0.20in}
	%%\resizebox{0.35 \linewidth}{!}{%
	%\scalebox{0.40}{%
%\input{riskret}
	%}%
%\end{center}%UNFOLD

<<'riskret',include=FALSE,fig=TRUE>>=

require(grDevices)

# set up population#FOLDUP
set.seed(118423)

#npnt <- 64
npnt <- 512
epy <- 253

#sr <- -0.6 + abs(rnorm(npnt,mean=0.5,sd=0.666))
sr <- rnorm(npnt,mean=0.3,sd=0.600)
xdf <- 5
sg <- 0.002 + 0.0090 * sqrt((1/xdf) * rchisq(npnt, xdf))
mu <- sg * sr / sqrt(epy)

# *very* optimistic this:
rfr <- 2.00
#UNFOLD

# plot func#FOLDUP
plot.all <- function(mu,sg,rfr,epy=253) {
	par(new=FALSE,usr=c(0,30,-20,40))

	xval <- 100 * sqrt(epy) * sg
	yval <-100 * epy * mu

	xat <- seq(0,40,by=10)
	yat <- seq(-30,50,by=15)


	# use xaxp and yaxp to control the x/y tick points?
	plot(100 * sqrt(epy) * sg,100 * epy * mu,
			 xlim=xat[c(1,length(xat))],
			 ylim=yat[c(1,length(yat))],
			 xaxt="n",yaxt="n",
			 xlab='annualized volatility',ylab='annualized return')
	abline(h=0,col='green',lty='dashed')

	tlab <- sapply(xat,function(dd) { sprintf('%g %%',dd) })
	axis(1,at=xat,label=tlab)

	tlab <- sapply(yat,function(dd) { sprintf('%g %%',dd) })
	axis(2,at=yat,label=tlab)

	points(0,rfr,type='p',col='red')

	x0 <- quantile(xval,probs=0.25)
	y0 <- quantile(yval,probs=0.76)
	x1 <- x0 + 2.1 * (quantile(xval,probs=0.01) - x0)
	#y1 <- y0 + 0.8 * (quantile(yval,probs=0.99) - y0)
	y1 <- y0 - (x1 - x0)

	# convex hull AKA efficient frontier#FOLDUP
	foohull <- chull(c(xval,max(xval),max(yval)),c(yval,max(yval),min(yval)))
	foohull <- foohull[-length(foohull)]
	foohull <- foohull[-length(foohull)]

	foook <- diff(xval[foohull]) >= 0
	tophull <- foohull[c(foook,TRUE)]

	#plot(xval,yval)
	lines(xval[tophull],yval[tophull],lwd=2,col='cyan')
	#UNFOLD
	
	arrows(x0,y0,x1,y1,lwd=2,col='blue',length=0.075)

	text(x0 + 0.55 * (x1-x0),
			 y0 + 0.9 * (y1-y0),
			 "preference",col='blue')

	#title(main="risk reward")
}#UNFOLD

plot.all(mu,sg,rfr,epy)

@

\vspace{-0.25in}
\begin{figure}[htbp]
  \begin{center}
		\includegraphics[width=2.0in]{figure/riskret}
		%\caption{Which strategy would you rather invest in? ``Slow and Steady'' or ``Loose Cannon''?}
		\label{fig:riskret}
	\end{center}
\end{figure}
\vspace{-0.20in}

\end{frame}
%UNFOLD

%\begin{frame}[allowframebreaks]{the Sharpe Ratio}
\begin{frame}{the Sharpe Ratio}%FOLDUP
\begin{itemize}
\item $\pmu,\psig$ are unknown; take sample estimates, \smu, \ssig.
%\item The Sharpe Ratio (SR) is the sample statistic
%$$\ssr \defeq \frac{\smu}{\ssig},$$
%where \smu is sample mean, \ssig sample standard deviation of strategy returns.\cite{RePEc:ucp:jnlbus:v:39:y:1965:p:119}
%May also include a 'risk-free' rate:
%$$\ssr = \frac{\smu - \rfr}{\ssig}.$$
\item The Sharpe Ratio (SR) is the sample statistic\cite{RePEc:ucp:jnlbus:v:39:y:1965:p:119}
$$\ssr \defeq \frac{\smu}{\ssig},$$
May also include a 'risk-free' rate:
$$\ssr = \frac{\smu - \rfr}{\ssig}.$$
\item Population analogue: Signal-to-Noise Ratio (SNR) $\psnr \defeq \pmu / \psig$.
\item Connection between SR and \tstat{}-statistic: $\ssr = \tstat{} / \sqrt{\ssiz}$.
\item SR is Student's original test statistic. \cite{student08ttest}
The `Student Ratio'?
%\end{itemize}
%\begin{itemize}
%\item $\ssr / \sqrt{\ssiz}$ asymptotically takes (non-central) \tlaw{}-distribution.
%\item A `natural arb': the \tlaw{}-distribution is well studied.  \cite{Johnson:1940,vanBelle2002_STRUTs}
\end{itemize}
\end{frame}
%UNFOLD

\begin{frame}[allowframebreaks]{Sharpe Ratio Minutiae}%FOLDUP
\begin{itemize}
\item No real standard on arithmetic vs. geometric returns.\\
(Although using arithmetic returns looks better ...)
\item The units of Sharpe are `per square root time':\\
\smu is `(percent) per time', \ssig is `(percent) per root time'.
\item Sharpe is often quoted in annualized units, \ie \yrto{-1/2}.\\
Avoid ambiguity and \emph{always} include units.
\item \nb $1\yrto{-1/2} = \frac{1}{2}\qto{-1/2} = \frac{1}{\sqrt{12}}\moto{-1/2} = \frac{1}{\sqrt{253}}\dayto{-1/2}$
\item For equities quant strategies, achieved SR translates as:
$1\yrto{-1/2} \Rightarrow \mbox{``good''}, 2\yrto{-1/2} \Rightarrow \mbox{``great''}, 3\yrto{-1/2} \Rightarrow \mbox{``legend''}.$\\
In HFT, higher SR the norm, but large fixed costs (like \rfr).

\break

<<'SR_equivalents'>>=
srs <- c(1,2,3,4)
prs <- pnorm(- srs,lower.tail=TRUE)
@

\item Cantelli's Inequality gives:
$$\Pr{\mbox{negative return on period}} \le \frac{1}{1 + {\psnr}^2}$$
(Often more natural to consider $\psrUL{2}{}, \ssrUL{2}{}$.)
\item Another view: a year-on-year loss is a ``\psnr sigma event''.\\
Central Limit Theorem: If returns are well behaved, 
enough gambles are made, annual return will be nearly normal.
\begin{align*}
\psnr = \Sexpr{srs[1]}\yrto{-1/2} &\Rightarrow \Pr{\mbox{down year}} = \Sexpr{signif(100 * prs[1],digits=2)}\mbox{\%}\\
\psnr = \Sexpr{srs[2]}\yrto{-1/2} &\Rightarrow \Pr{\mbox{down year}} = \Sexpr{signif(100 * prs[2],digits=2)}\mbox{\%}\\
\psnr = \Sexpr{srs[3]}\yrto{-1/2} &\Rightarrow \Pr{\mbox{down year}} = \Sexpr{signif(100 * prs[3],digits=2)}\mbox{\%}\\
\psnr = \Sexpr{srs[4]}\yrto{-1/2} &\Rightarrow \Pr{\mbox{down year}} = \Sexpr{signif(100 * prs[4],digits=2)}\mbox{\%}
\end{align*}
%2FIX: How does this translate to quarters?

\end{itemize}
\end{frame}
%UNFOLD

\begin{frame}{Diversification and Sharpe Ratio}%FOLDUP
\begin{itemize}
\item \emph{Squared} SNR of independent strategies is subadditive:
For $k$ \coindie strategies, with SNRs $\psnr[1],\ldots,\psnr[k]$, optimal
rebalancing combination has
$$\psrsq[*] = \sum_i \psrsq[i]$$
Diminishing returns in \psr : $2\yrto{-1/2} + 1\yrto{-1/2} \Rightarrow \Sexpr{signif(sqrt(2^2 + 1),digits=2)}\yrto{-1/2}$.
\item For dependent strategies, correlation is relevant.\\
For two strategies with correlation $\rho$:
$$\psrUL{2}{*} = \frac{\psrUL{2}{1} + \psrUL{2}{2} - 2\rho\psrUL{}{1}\psrUL{}{2}}{1 - \rho^2}$$
\item In general, want anti-correlated strategies (`hedges').\\
Avoid strategies correlated to the `retail alpha'.
\end{itemize}
\end{frame}
%UNFOLD

\begin{frame}{Diversification vs Correlation}%FOLDUP

<<'testit',include=FALSE,fig=TRUE>>=
@

<<'divandcorr',include=FALSE,fig=TRUE>>=
#snr1 <- 1 / sqrt(2)
#snr2 <- snr1
#corrs <- seq(from=-0.5,to=0.5,length.out=1001)

snr1 <- 0.5
snr2 <- sqrt(1 - snr1^2)
corrs <- seq(from=-0.5,to=0.95,length.out=1001)
snrs <- sqrt((snr1^2 + snr2^2 - 2*corrs*snr1*snr2) / (1-corrs^2))
# where does the snrstar 'bottom out'?
corr.worst <- min(snr1/snr2,snr2/snr1)
snrboo <- sqrt((snr1^2 + snr2^2 - 2*corr.worst*snr1*snr2) / (1-fooz^2))
#snrboo - max(snr1,snr2)
corr.bpoint <- 2*snr1*snr2 / (sum(c(snr1,snr2)^2))

plot(corrs,snrs,xlab="correlation",ylab="optimal SNR",type='l',col='green')
abline(h=norm(matrix(c(snr1,snr2)),"2"),col='gray',lty=2)
abline(v=0,col='gray',lty=2)
abline(h=max(snr1,snr2),col='gray',lty=2)
abline(v=corr.worst,col='gray',lty=2)
abline(v=corr.bpoint,col='gray',lty=2)
@
\vspace{-0.35in}
\begin{figure}[htbp]
  \begin{center}
		\includegraphics[width=2.5in]{figure/divandcorr}
		\caption{\psnr[*] is plotted versus $\rho$ for
			$\psnr[1] = \Sexpr{signif(snr1,digits=2)}\yrto{-1/2}, \psnr[2] = \Sexpr{signif(snr2,digits=2)}\yrto{-1/2}$.
			It is possible to get \emph{no} diversification benefit beyond best strategy.}
		\label{fig:divandcorr}
	\end{center}
\end{figure}
\vspace{-0.20in}
\end{frame}
%UNFOLD

%UNFOLD

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Statistics of Sharpe Ratio}%FOLDUP

\begin{frame}[allowframebreaks]{Inference on Sharpe Ratio}%FOLDUP
\begin{itemize}
\item SR is actually a \emph{biased} estimator of SNR:\cite{CambridgeJournals:4493808}
$$
\E{\ssr} = \sqrt{\frac{\ssiz-1}{2}}\frac{\GAM{(\ssiz-2)/2}}{\GAM{(\ssiz-1)/2}} \psr = \frac{1}{c_4}\psr.
$$
Though the bias is small ($\le \Sexpr{signif(100 * (f_tbias(100-1)-1),digits=2)} \%$ when $\ssiz \ge 100$).
\item Standard error assuming normal returns: \cite{walck:1996,lo2002,Johnson:1940}
$$
s.e. = \sqrt{\frac{1 + \frac{\psrsq}{2}}{\ssiz - 1}}
\approx \sqrt{\frac{1 + \frac{\ssrsq}{2}}{\ssiz - 1}}
\approx \sqrt{\frac{1}{\ssiz}}.
$$
The latter annualizes as you would expect.
\\Rule of thumb: $\abs{\psr - \ssr} \le \frac{2}{\sqrt{\ssiz}}$, with probability $\approx 0.95$.

\break

\item Fixes for heteroskedasticity, autocorrelation, skew.
\cite{vanBelle2002_STRUTs,lo2002,Opdyke2007}
(Typically these bias \ssr by $< 10\%$)
\item Bigger problem is omitted variable bias \ie ``something changed in the world.''
The ``unknown unknowns.''
\item Even bigger problem: how do you set the type I rate? The incidence rate of profitable
strategies is unknown:
\end{itemize}

\begin{center}
\begin{tabular}{|l||c|c|}\hline
	what & profitable ($\irate$) & not profitable ($1 - \irate$)\\\hline
	reject null &  $\irate (1-\typeII)$ & $(1-\irate) \typeI$ \\
	fail to reject & $\irate \typeII$ & $(1-\irate) (1 - \typeI)$ \\\hline
\end{tabular}
%$$\mbox{thus,}\quad\mbox{FDR} \ge 1 - \frac{\irate}{2\irate + \typeI} \approx 1 - \frac{\irate}{\typeI}.$$
$$\mbox{thus,}\quad\mbox{FDR} \gtrsim 1 - \frac{\irate}{\typeI}.$$
\end{center}


\end{frame}%UNFOLD

<<'t_power_rules_brief'>>=
rhovals <- seq(0.5,3.0,by=0.10)
rhovals_day <- rhovals / sqrt(dpy)
# 1-sided test
samps50.1 <- (1/dpy) * unlist(lapply(rhovals_day, f_treqsize, powr = 0.50))
samps80.1 <- (1/dpy) * unlist(lapply(rhovals_day, f_treqsize, powr = 0.80))
coin.flip.c <- median(samps50.1 * rhovals**2)
hi.conf.c <- median(samps80.1 * rhovals**2)
@

\begin{frame}[allowframebreaks]{Power and Sample Size}%FOLDUP
Relation between sample size, effect size (SNR), and rates of 
false positives and false negatives.\\
Figure out if you have enough data or enough 'alpha'.
\begin{itemize}
	\item Hypothetical Vendor: ``I have three years of historical data.''
	\item Hypothetical Strategist: ``This strategy has SNR $0.6 \yrto{-1/2}$.''
	\item Hypothetical Investor: ``You have one year to prove yourself.''
\end{itemize}

Good approximations of form
$$\ssiz \approx \frac{\tpowc}{\psnrsq},$$
with \tpowc a function of type I and type II rates, \etc
\cite{vanBelle2002_STRUTs,Johnson:1940}

<<'t_power_rules',print=TRUE,results="asis">>=
require(xtable)
rhovals <- seq(0.5,3.0,by=0.10)
rhovals_day <- rhovals / sqrt(dpy)
samps50.1 <- (1/dpy) * unlist(lapply(rhovals_day, f_treqsize, powr = 0.50))
samps80.1 <- (1/dpy) * unlist(lapply(rhovals_day, f_treqsize, powr = 0.80))
#2sided test
samps50.2 <- (1/dpy) * unlist(lapply(rhovals_day, f_treqsize, alpha=0.025, powr = 0.50))
samps80.2 <- (1/dpy) * unlist(lapply(rhovals_day, f_treqsize, alpha=0.025, powr = 0.80))
foo <- data.frame("one sided" = c(median(samps50.1 * rhovals**2),median(samps80.1 * rhovals**2)),
									"two sided" = c(median(samps50.2 * rhovals**2),median(samps80.2 * rhovals**2)),
									row.names = c("power = 0.50","power = 0.80"))
xres <- xtable(foo,label="tab:ttestpower",caption="Value of \\tpowc to achieve given power in t-test, $\\typeI = 0.05$.")
print(xres,include.rownames=TRUE)
coin.flip.c <- median(samps50.1 * rhovals**2)
hi.conf.c <- median(samps80.1 * rhovals**2)
@

\break

\begin{alertblock}{rule of thumb for Sharpe power}
To test SNR $> 0$, with 0.05 type I rate, and 50 \% power, 
$$\ssiz \approx \frac{\Sexpr{signif(coin.flip.c,digits=3)}}{\psnrsq}.\qquad\mbox{mnemonic form: }e \approx \ssiz\psnrsq$$
\end{alertblock}

This rule is sobering:
\begin{itemize}
	\item Hypothetical Vendor: ``I have three years of historical data.''\\
	Answer: Strategy must have SNR 
	$\ge \Sexpr{signif(sqrt(coin.flip.c / 3),digits=2)} \yrto{-1/2}$.
	\item Hypothetical Strategist: ``This strategy has SNR $0.6 \yrto{-1/2}$.''\\
	Answer: Need 
	\Sexpr{signif(coin.flip.c / (0.6 ^ 2),digits=2)} years of data to backtest. (!)
	\item Hypothetical Investor: ``You have one year to prove yourself.''\\
	Answer: Strategy must have SNR $\ge \Sexpr{signif(sqrt(coin.flip.c / 1),digits=2)} \yrto{-1/2}$.
\end{itemize}

\end{frame}
%UNFOLD
%abbreviated version. still too long. fuck it.
%\begin{frame}[allowframebreaks]{Power and Sample Size}%FOLDUP
%Power: Relation between sample size, effect size (SNR), and rates of 
%false positives and false negatives. ``Do I have enough data or alpha?''
%\begin{alertblock}{rule of thumb for Sharpe power}
%To test SNR $> 0$, with 0.05 type I rate, and 50 \% power, 
%$$\ssiz \approx \frac{\Sexpr{signif(coin.flip.c,digits=3)}}{\psnrsq}.\qquad\mbox{mnemonic form: }e \approx \ssiz\psnrsq$$
%\end{alertblock}

%This rule is sobering:
%\begin{itemize}
	%%\item Hypothetical Vendor: ``I have three years of historical data.''\\
	%%Answer: Strategy must have SNR 
	%%$\ge \Sexpr{signif(sqrt(coin.flip.c / 3),digits=2)} \yrto{-1/2}$.
	%\item Hypothetical Strategist: ``This strategy has SNR $0.7 \yrto{-1/2}$.''\\
	%Rule: Need 
	%\Sexpr{signif(coin.flip.c / (0.7 ^ 2),digits=2)} years of data to backtest. (!)
	%\item Hypothetical Investor: ``You have one year to prove yourself.''\\
	%Rule: Strategy must have SNR $\ge \Sexpr{signif(sqrt(coin.flip.c / 1),digits=2)} \yrto{-1/2}$.
%\end{itemize}

%\end{frame}
%%UNFOLD


%UNFOLD

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{QuantFail}%FOLDUP

\begin{frame}[allowframebreaks]{Why do Bad Things Happen to Smart People?}
\vspace{-0.35in}
\begin{figure}[htbp!]
  \begin{center}
		\includegraphics[width=2.5in]{figure/Curve_fitting}
		\caption{``The majority of deployed quant strategies are type I errors.''\relax\break
			{\scriptsize (Image courtesy of Automated Trader magazine.)}}
		\label{fig:overfit}
	\end{center}
\end{figure}
\vspace{-0.20in}

\break
\begin{itemize}
\item Bad backtests. (``backtest arbitrage'')
\item Biased statistical tests.
\item ``non-stationarity'' (\ie ``the dog ate my alpha.'')
\end{itemize}

\end{frame}


\begin{frame}[allowframebreaks]{What do Quants do?}%FOLDUP

% Define block styles
\tikzstyle{decision} = [diamond, draw, fill=blue!20, 
    text width=4.5em, text badly centered, node distance=3.3cm, inner sep=0pt]
\tikzstyle{block} = [rectangle, draw, fill=blue!20, 
    text width=5em, text centered, rounded corners, minimum height=4em]
\tikzstyle{line} = [draw, -latex']
\tikzstyle{cloud} = [draw, ellipse,fill=red!20, node distance=3.3cm,
    minimum height=2em]
    
\begin{center}%FOLDUP
	How it is supposed to happen:
	\vspace{0.20in}

	%\resizebox{0.35 \linewidth}{!}{%
	\scalebox{0.45}{%
		\begin{tikzpicture}[node distance = 3.3cm, auto]
			% Place nodes
			\node [block] (test) {backtest a strategy.};
			\node [decision, below of=test] (decide) {pass statistical tests?};
			\node [block, below of=decide] (giveup) {become an accountant.};
			\node [block, right of=decide] (trade) {trade it.};
			\node [block, below of=trade] (profit) {profit.};
			% Draw edges
			\path [line] (test) -- (decide);
			\path [line] (decide) -- node{yes} (trade);
			\path [line] (decide) -- node{no} (giveup);
			\path [line] (trade) -- (profit);
		\end{tikzpicture}
	}%
\end{center}%UNFOLD

\break

\begin{center}%FOLDUP
	Or even this:
	\vspace{0.20in}

	%\resizebox{0.35 \linewidth}{!}{%
	\scalebox{0.45}{%
		\begin{tikzpicture}[node distance = 3.3cm, auto]
			% Place nodes
			\node [block] (test) {backtest many strategies.};
			\node [block, below of=test] (mht) {apply Bonferroni correction.};
			\node [decision, below of=mht] (decide) {pass statistical tests?};
			\node [block, below of=decide] (giveup) {become an accountant.};
			\node [block, right of=decide] (trade) {trade it.};
			\node [block, below of=trade] (profit) {profit.};
			% Draw edges
			\path [line] (test) -- (mht);
			\path [line] (mht) -- (decide);
			\path [line] (decide) -- node{yes} (trade);
			\path [line] (decide) -- node{no} (giveup);
			\path [line] (trade) -- (profit);
		\end{tikzpicture}
	}%
\end{center}%UNFOLD

\break

\begin{center}%FOLDUP
	How it typically happens:
	\vspace{0.20in}

	%\resizebox{0.35 \linewidth}{!}{%
	\scalebox{0.45}{%
		\begin{tikzpicture}[node distance = 3.3cm, auto]
			% Place nodes
			\node [block] (test) {backtest many strategies.};
			\node [block, below of=test] (mht) {apply Bonferroni correction.};
			\node [decision, below of=mht] (decide) {pass statistical tests?};
			\node [block, below of=decide] (refine) {refine ideas based on results};
			% invisible node helpful later
			\node[left of=mht,scale=0.05] (inv) {};

			\node [block, right of=decide] (trade) {trade it.};
			\node [block, below of=trade] (profit) {profit (?)};
			% Draw edges
			\path [line] (test) -- (mht);
			\path [line] (mht) -- (decide);
			\path [line] (decide) -- node{yes} (trade);
			\path [line] (decide) -- node{no} (refine);
			\path [line] (trade) -- (profit);
			% \path [line] (refine) -| (test);
      \path[-,draw] (refine) -| node{} (inv);
      \path[line] (inv) |- (test);
		\end{tikzpicture}
	}%

	\vspace{0.10in}
	This is totally broken.
\end{center}%UNFOLD

\end{frame}%UNFOLD

\begin{frame}{Why does overfit happen?}
\begin{itemize}

\item There is only one history to work with.
\item We collect data at a rate of 1 day per day.
\item Per the power rules, you need \emph{years} of data to confirm an effect.
\item The incidence rate is very low.
\item Human nature to confuse noise for signal.
\item Sloppy process.

\end{itemize}
\end{frame}



%UNFOLD
	
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Markowitz \& Hotelling}%FOLDUP

\begin{frame}{Multivariate Generalization: Hotelling \& Markowitz}%FOLDUP

\begin{itemize}
\item Consider \ssiz observations of \nlatf-vector of asset returns.
\item Hotelling \Tstat is multivariate generalization of \tstat:
\begin{equation}
\nonumber
\begin{split}
\Tstat &\defeq \ssiz {\tr{\svmu}\svsig^{-1}\svmu},\\
 &= \ssiz \max_{\sportw} \wrapNeParens{\frac{\tr{\sportw}\svmu}{\sqrt{\tr{\sportw}\svsig \sportw}}}^2 
%= \ssiz \max_{\sportw} \funcit{\ssrsq}{\sportw} 
= \ssiz \ssrsq[*],
\end{split}
\end{equation}
where \ssr[*] is SR of Markowitz portfolio $\sportw[*]\propto\minv{\svsig}\svmu$.\cite{Hotelling:1931,Sen20073741}  % Roy's UIT
\item Via Hotelling \Tstat, maximal Sharpe squared has \flaw{}-distribution:
$$
\frac{(\ssiz - \nlatf)}{\nlatf (\ssiz - 1)} \ssiz \ssrsq[*] \sim \flaw{\ssiz\psnrsq[*],\nlatf,\ssiz - \nlatf},
$$
where \psnr[*] is SNR of \emph{population} optimal portfolio, $\pportw[*]\propto\minv{\pvsig}\pvmu$.\\
So \ssr[*] can be used to perform inference on \psnr[*]!
\item \emph{n.b.} \psnr[*] is maximal SNR of \emph{any} portfolio, including \sportw[*].
\end{itemize}
\end{frame}
\nocite{Sen20073741}  % Roy's UIT
\nocite{kansmith2008}
%UNFOLD

\begin{frame}{Inference on \psnr[*]}%FOLDUP
\begin{itemize}
\item Quant 101: ``don't look at \ssr[*] when computing \sportw[*].''
%(\ie worse than uninformative, actually harmful.)
\item Via \flaw{}-distribution we have
%\begin{equation}
%\nonumber
%\begin{split}
%\E{\ssrsq[*]} &= \frac{\psnrsq[*] + \arat}{1 - \arat},\qquad\mbox{where}\,\,\arat \defeq \nlatf / \ssiz,\\
%\ssiz\VAR{\ssrsq[*]} &\approx \frac{2\wrapNeParens{\arat + 2\psnrsq[*]}}{(1-\arat)^3}.
%\end{split}
%\label{eqn:srstarstats}
%\end{equation}
\begin{equation}
\nonumber
\E{\ssrsq[*]} = \frac{\psnrsq[*] + \arat}{1 - \arat},\qquad\mbox{where}\,\,\arat \defeq \nlatf / \ssiz.
\label{eqn:srstarstats}
\end{equation}
%Gives unbiased estimator: $\E{(1-\arat)\ssrsq[*] - \arat} = \psnrsq[*]$.
\item CI, MLE on \psnr[*] via \flaw{}-CDF \& PDF (\texttt{pf} and \texttt{df}).
%\item Via \flaw{}-distribution, if $\ssrsq[*] \le \frac{\arat}{1 - \arat}$ then MLE of $\psnr[*]$ is 0. \cite{MC1986216}
\item `Aspect ratio' $\arat\defeq\nlatf / \ssiz$ sets the bar for \ssrsq[*].
\end{itemize}
\vspace{-0.03in}
\begin{alertblock}{rule of thumb, based on \flaw{}-MLE}
``If $\ssrsq[*] < \arat$, don't trade it!''
\end{alertblock}
\nocite{MC1986216}
\end{frame}
%UNFOLD

% USAGE of vanilla Hotelling%FOLDUP
<<'vanillaT'>>=
library(fPortfolio)

lr.to.rr <- function(lr) {
	return(exp(lr) - 1)
}

optimal.sr2 <- function(rets) {
	mu.hat <- as.vector(apply(rets,MARGIN=2,mean,na.rm=TRUE))
	Sig.hat <- cov(rets)
	w.opt <- solve(Sig.hat,mu.hat)
	return(t(mu.hat) %*% w.opt)
}

hotelling.T2 <- function(rets) {
	return(dim(rets)[1] * optimal.sr2(rets))
}

delta.hotelling.T2 <- function(rets,pcols) {
	Tp <- hotelling.T2(rets[,pcols])
	Tpq <- hotelling.T2(rets)

	n <- dim(rets)[1]
	pq <- dim(rets)[2]
	p <- length(pcols)
	q <- pq - p

	delta.T <- (n - p - 1) * (Tpq - Tp) / (n - 1 + Tp)
	as.F <- ((n - pq) / (q * (n - p - 1))) * delta.T

	# inference on the delta T
	delta.ci <- fncp.ci(as.F,df1=q,df2=n-pq)
	delta.mle <- fncp.mle(as.F,df1=q,df2=n-pq)
	as.sr <- sqrt(delta.T / n)

	return(list("deltaT"=delta.T,
							"asF"=as.F,
							"ci"=delta.ci,
							"mle"=delta.mle))
}


hotelling.example <- function(rets) {
	sr.opt <- sqrt(optimal.sr2(rets))
	# now run some inferential shit on it!
	n <- dim(rets)[1]
	p <- dim(rets)[2]
	return(list("opt"=sr.opt,
							"ci"=srstarncp.ci(sr.opt,p=p,n=n),
							"mle"=srstarncp.mle(sr.opt,p=p,n=n)))
}

#x.name <- "SMALLCAP"
#x.data <- lr.to.rr(SMALLCAP.RET)
#x.name <- "SWX"
#x.data <- lr.to.rr(SWX.RET[,1:4])

x.name <- "SPISECTOR"
x.data <- lr.to.rr(SPISECTOR.RET)
# cut out 'SPI'
x.data <- x.data[,2:(dim(x.data)[2])]

x.run <- hotelling.example(rets=x.data)
x.dpy <- 253
n <- dim(x.data)[1]
pq <- dim(x.data)[2]
arat <- pq / n

x.rown <- rownames(x.data)
x.TEO1 <- x.rown[1]
x.TEOf <- x.rown[length(x.rown)]

## delta hotelling#FOLDUP
n <- dim(x.data)[1]
pq <- dim(x.data)[2]


#pcols <- c(1:7)
#pcols <- c(1,2,3,4,5,7)
#pcols <- c(1,2,3,4,5)
#pcols <- c(6,7,8,9)
pcols <- c(7,8,9)
qcols <- setdiff((1:pq),pcols)

p <- length(pcols)
q <- pq - p

dh.run <- delta.hotelling.T2(x.data,pcols)

# annualize them by multiplying by x.dpy, not sqrt
dd.cilo <- dh.run$ci$lo * (x.dpy) / (n - p - 1)
dd.cihi <- dh.run$ci$hi * (x.dpy) / (n - p - 1)
dd.mle <- dh.run$mle * (x.dpy) / (n - p - 1)

x.names <- colnames(x.data)
pq.names <- x.names
p.names <- x.names[pcols]
q.names <- x.names[qcols]
#UNFOLD
@
%UNFOLD

\begin{frame}{Hotelling on S\&P Sector Indices}%FOLDUP
Index data \texttt{\Sexpr{paste(pq.names,sep=" ",collapse=", ")}} from 
\texttt{\Sexpr{paste("fPortfolio::",x.name,sep="")}},
from \Sexpr{x.TEO1} to \Sexpr{x.TEOf}.  ($\ssiz = \Sexpr{n}$ days, $\nlatf = \Sexpr{pq}$ stocks)
\begin{itemize}
\item Optimal in-sample Sharpe ratio is $\Sexpr{signif(sqrt(x.dpy) * x.run$opt,digits=2)}\yrto{-1/2}$.
\item MLE for $\psnr[*]$ is $\Sexpr{signif(sqrt(x.dpy) * x.run$mle,digits=2)}\yrto{-1/2}$.
%(Note $\ssrsq[*] = \Sexpr{signif(x.run$opt ^ 2,digits=3)}\dayto{-1}$ and $\arat / (1-\arat)= \Sexpr{signif(arat / (1-arat),digits=3)}$, close to the rule of thumb cutoff.)
%Note $\ssrsq[*] = \Sexpr{signif(x.run$opt ^ 2,digits=3)}\dayto{-1}$ and 
%$\arat = \Sexpr{signif(arat,digits=3)}\dayto{-1}$, 
%close to the rule of thumb cutoff.
Close to the rule of thumb cutoff:
$\ssrsq[*] = \Sexpr{signif(x.dpy * x.run$opt ^ 2,digits=3)}\yrto{-1}$ and 
$\arat = \Sexpr{signif(x.dpy * arat,digits=3)}\yrto{-1}$.
\item 95\% CI for $\psnr[*]$ is $\wrapNeBracks{\Sexpr{signif(sqrt(x.dpy) * x.run$ci$lo,digits=2)}\yrto{-1/2},\Sexpr{signif(sqrt(x.dpy) * x.run$ci$hi,digits=2)}\yrto{-1/2}}$
\end{itemize}
Conclusion: Markowitz portfolio on S\&P sectors not recommended.\\
(joke: how many 'F's are there in 'alpha'?)
\end{frame}
%UNFOLD

%UNFOLD

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Odds \& Ends}%FOLDUP

\begin{frame}[allowframebreaks]{Approximation of Strategy Overfit}%FOLDUP
Caricature of quant work:
\begin{itemize}
\item Construct strategy with free parameters \stratrc;
\item Backtest strategy for $\stratrc[1],\stratrc[2],\ldots,\stratrc[m]$.
\item Pick \stratrc[i] that maximizes SR of backtest, $\stratrc[*]$.% = \mbox{argmax}_{1\le i\le m}\funcit{\ssr}{\stratrc[i]}$
\item Profit! (or not)
\end{itemize}

Toy Example: Moving Average Crossover:
\begin{itemize}
\item \stratrc is vector of 2 window lengths; Long the instrument
exactly when one moving average exceeds the other.
\item Brute-force backtest for allowable window lengths.
\end{itemize}
Q: How to estimate the SNR of \stratrc[*]?

\break
Q: How to estimate the SNR of \stratrc[*]?\\
A(?): Make PCA-like linear approximation of returns vectors:
%$$\wrapNeBraces{\vreti[1],\ldots,\vreti[m]} \approx \mathcal{L} \subset \setwo{\sum_{1\le j \le \nlatf} k_j \vretj[j]}{k_j \in \reals}$$
$$\wrapNeBraces{\vreti[1],\ldots,\vreti[m]} \approx \mathcal{K} \subset \mathcal{L} \defeq \setwo{\mretj \sportw}{\sportw \in \reals{\nlatf}}$$
Use $\funcit{\psnr}{\stratrc[*]} \approx \max_{\mathcal{L}} \psnr = \psnr[*]$,
then make inference on \psnr[*].

Observe maximal strategy SR
$$\ssr[*] = \funcit{\ssr}{\stratrc[*]} \defeq \max_{1\le i \le m} \funcit{\ssr}{\stratrc[i]},$$
use it to approximate maximal SR over $\mathcal{L}$.

You have to estimate \nlatf, by Monte Carlo under null, PCA, or SWAG method.
\end{frame}
%UNFOLD

\input{mac_overfit}


\begin{frame}{Further Directions}%FOLDUP
\begin{itemize}
\item Version 0.01 of \texttt{ratarb} package.
\item Markowitz portfolio \sportw[*] does not achieve \psnr[*]; model the gap.
\item Validate the overfit approximations 'under the alternative'.
\item Problems under constrained portfolios; \Tstat[+] is \emph{not} a 'similar' statistic;
distribution depends on nuisance parameter \pvsig. \cite{Silvapulle1996137,Sen1999264,nla.cat-vn3800977}
\end{itemize}
\end{frame}
%UNFOLD

%UNFOLD

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Appendix}%FOLDUP

\begin{frame}[allowframebreaks]{Bibliography}
\tiny
%bibliography%FOLDUP

%%generate the rauto.bib bitex file%FOLDUP
%see also
%http://r.789695.n4.nabble.com/Automating-citations-in-Sweave-td872079.html
<<'gen_bibliography'>>=
FID <- file("rauto.bib", "w")  # open an output file connection

cite.by.name <- function(x){ 
	res <- toBibtex(citation(x)) 
	if (is.list(res)) res <- res[[1]] 
	res[1] <- sub("{",paste("{",x,sep=''),res[1],fixed=TRUE) 
	#2FIX: multiple citations; bleah;
	cat(res,file = FID, sep = "\n")
	return(NULL)
} 
z <- sapply( .packages(TRUE), function(x) try( cite.by.name(x) ) )
close(FID)
@
%UNFOLD

\bibliographystyle{plain}
%\bibliographystyle{sepbib}
\bibliography{dude,rauto}
%\bibliography{sepbiblio}
%UNFOLD
\end{frame}

\begin{frame}[allowframebreaks]{Skew and Sharpe}%FOLDUP
\small
\input{skew_study_show}
\end{frame}%UNFOLD

\begin{frame}{Wait, I Want Returns}%FOLDUP

<<'ddown_connection'>>=
source("drawdown.R")
epy <- 253

x.psis <- seq(0,2.5/sqrt(epy),length.out=32)
dprob <- 0.975

gen.per <- ifelse(FINAL.VERSION,2^18,2^15)

y.dds <- sapply(x.psis,function(sr) { 
	zzz <- rdd(gen.per,epy,sr,tgen)
	retval <- quantile(zzz,probs=dprob,type=7)[[1]]
})

@

<<'ddown_plot',include=FALSE,fig=TRUE>>=

# in percent
ddown.max <- 33
x.psi.pa <- sqrt(epy) * x.psis
y.mxv <- - log(1 - (ddown.max/100)) / y.dds
y.mxv.pa <- sqrt(epy) * y.mxv
y.mxv.pretty <- 100 * lr2rr(y.mxv.pa)

plot(x.psi.pa,y.mxv.pretty,xlab="S.R. (yr^{-1/2})",ylab="max volatility (% yr^{-1/2})",type='p',col='blue')
abline(lm(y.mxv.pretty ~ x.psi.pa))

@
Idea: lever up until volatility ``too high.'' Set this by a drawdown cap. Sharpe Ratio limits drawdowns.
\vspace{-0.35in}
\begin{figure}[htbp]
  \begin{center}
		\includegraphics[width=2.0in]{figure/ddown_plot}
		\caption{Maximum volatility (percent per root year) so that probability of a \Sexpr{ddown.max} \%
			drawdown over a year is \Sexpr{100 * (1-dprob)}\% or less. Assumes independence, homoskedasticity,
			mild kurtosis.}
		\label{fig:ddown}
	\end{center}
\end{figure}
\vspace{-0.20in}

\end{frame}%UNFOLD

\begin{frame}{Delta Hotelling}%FOLDUP

\begin{itemize}
\item Can generalize Hotelling statistic to get 'Spanning Tests'
\cite{BrittenJones1999,KanZhou2012}
\item Let \Tstat[\nlatftot],\Tstat[\nlatf] be Hotelling stats on 
full set of \nlatftot assets and subset of $\nlatf$ assets.
Do the \nlatfmo  marginal assets 'add any value'.
\item Let
$$
\Delta \Tstat = (\ssiz - \nlatf - 1)\frac{\Tstat[\nlatftot]-\Tstat[\nlatf]}{\ssiz - 1 + \Tstat[\nlatf]} = 
(\ssiz - \nlatf - 1) \frac{\ssrsq[*,\nlatftot] - \ssrsq[*,\nlatf]}{1 - (1/\ssiz) + \ssrsq[*,\nlatf]}.
$$
\item $\Delta\Tstat$ also takes a (non-central) Hotelling distribution:
\begin{equation}
\nonumber
\begin{split}
\frac{\ssiz-(\nlatftot)}{\nlatfmo(\ssiz-\nlatf-1)} \Delta \Tstat &\sim \flaw{\nlatfmo,\ssiz-(\nlatftot),\ncfp},\\
		\ncfp &= (\ssiz - \nlatf - 1) \frac{\psnrsq[*,\nlatftot] - \psnrsq[*,\nlatf]}{1 - (1/\ssiz) + \psnrsq[*,\nlatf]}.
\end{split}
\end{equation}
\item Same inference on \ncfp can be applied (MLE, CI).
\end{itemize}
\end{frame}
%UNFOLD

\begin{frame}{Delta Hotelling on S\&P Sector Indices}%FOLDUP
Delta Hotelling: what do \texttt{\Sexpr{paste(q.names,sep=" ",collapse=", ")}} add to 
\texttt{\Sexpr{paste(p.names,sep=" ",collapse=", ")}}?
\begin{itemize}
\item MLE for $\wrapNeParens{\psnrsq[*,\nlatftot] - \psnrsq[*,\nlatf]}$ is 
$\Sexpr{signif(dd.mle,digits=2)}\yrto{-1}$.
\item 95\% CI for $\wrapNeParens{\psnrsq[*,\nlatftot] - \psnrsq[*,\nlatf]}$ is
$\wrapNeBracks{\Sexpr{signif(dd.cilo,digits=2)}\yrto{-1},\Sexpr{signif(dd.cihi,digits=2)}\yrto{-1}}$.
\end{itemize}
\end{frame}
%UNFOLD

%\begin{frame}[fragile]{Some R Code}%FOLDUP
%%\begin{lstlisting}[language=R,basicstyle=\footnotesize\ttfamily,caption=CI on SNR]
%{\footnotesize
%\begin{verbatim}
%p.sr <- function(sr,df,snr=0,a.fact=252) {
	%#H_0: SNR=given snr vs H_1: SNR>given snr
	%p.val <- pt(sr*sqrt(df/a.fact),df-1,
							%snr*sqrt(df/a.fact),lower.tail=FALSE)
%}
%qco.sr <- function(sr,df,alpha,a.fact=252) {
	%#q'tile of S.R. 'Confidence Distribution'
	%adj.c <- sqrt(df/a.fact)
	%find.ncp <- uniroot(function(ncp)
				%(pt(sr*adj.c,df-1,ncp) - (1-alpha)),
				%interval=c(-36,36)) # a hack!
	%return(find.ncp$root / adj.c)
%}
%sr.ci <- function(sr,df,alpha,...) {  # CI
	%return(c(qco.sr(sr,df,alpha/2,...),
					 %qco.sr(sr,df,1-(alpha/2),...)))
%}
%\end{verbatim}
%}
%%\end{lstlisting}
%\end{frame}%UNFOLD
%UNFOLD

%see http://robfelty.com/2008/09/22/beamer-fragile-frames 
%for fragile and verbatim
%\begin{frame}[fragile]{Facts about \tstat{} are facts about SR}%FOLDUP
%\begin{itemize}
%\item A `natural arb': the \tlaw{}-distribution is well studied.
%\item Standard error for SR same as that for \tstat{}.
%\cite{lo2002,Johnson:1940}
%%%\fbox{ \includegraphics[width=1.2in]{dude_show_Lo_serr_1.eps} }
%%%\fbox{ \includegraphics[width=0.5in]{dude_show_johnson_welch_serr_1.eps} }
%%\begin{center}
%%\fbox{ \includegraphics[width=1.2in]{dude_show_Lo_serr_2.eps} }
%%\fbox{ \includegraphics[width=0.5in]{dude_show_johnson_welch_serr_3.eps} }
%%\end{center}
%\item Correction for autocorrelation in \tstat{} apples to SR.  \cite{vanBelle2002_STRUTs}
%\item CI via inverting \tlaw{}-pdf for non-centrality.
%\end{itemize}
%\end{frame}
%%UNFOLD

\end{document}
%for vim modeline: (do not edit)
% vim:ts=2:sw=2:tw=79:fdm=marker:fmr=FOLDUP,UNFOLD:cms=%%s:syn=rnoweb:ft=rnoweb:ai:si:cin:nu:fo=croql:cino=p0t0c5(0:
